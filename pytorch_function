torch.nn.Linear(in_features, out_features, bias=True)
  >in_features:每个输入样本的大小
  >out_features:每个输出样本的大小
  >bias:默认学习偏置，若设置false则该层不学习偏置

torch.nn.Conv2d(in_channels, out_channels, kernal_size, stride=1,padding=0, dilation=1, groups=1, bias=True)
  >in_channels(int)——输入信号的通道
  >out_channels(int)——卷积产生的通道
  >kerner_size(int,truple)——卷集核的尺寸
  >stride（int,truple,optional）——卷积步长

torch.nn.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, retrun_indices=False)
  >input(minibatch*in_channels*iH*iW):输入的张量
  >kernal_size(int,truple):池化区域的大小
  >stride(int,truple):池化操作步长，默认等于核的大小
  >padding(int,truple):在输入上隐式的零填充，默认0
  >ceil_mode:定义空间爱你输出形状的操作
  >count_include_pad(int,truple):除以原始非填充图像内的元素

x = torch.randn(4,4)
y = x.view(2,8)
y = x.view(-1,8)
y = x.view(16)
view(*args):返回具有相同数据但大小不同的新张量。返回的张量必须有与原张量相同的数据和相同数量的元素，但可以有不同的大小

torch.mean(x):返回所有x的平均值

torch.utils.data.Dataloader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=<function default_collate>, pin_money=False, drop_last=False)
  >dataset(Dataset):从中加载数据的数据集
  >batch_size(int,optional):批训练的数据个数（默认:1）
  >shuffle(bool,optional):设置为True在每个epoch重新排列数据（默认：False,一般打乱比较好）
  >sampler(Sampler,optional):定义从数据集中提取样本的策略。如果指定，则忽略shuffle参数
  >batch_sampler(sampler,可选)：和sampler一样，但一次返回一批索引。与batch_size, shuffle, sampler和drop_last相互排斥
  >num_workers(int,可选):用于数据加载的子进程数，0表示数据将在主进程中加载（默认值：0）

torchuvision.datasets.CIFAR10(root, train=True, transform=None, target_transform=None, download=False)
  >root:cifar-10-batches-py根目录
  >train:True=训练集，False=测试集
  >download:True=从互联网下载数据，并将其放在root目录下。选择False不下载，什么都不干
  >transform(可调用，可选):接收PIL映像并返回转换版本的函数
  >target_transfrom(可调用，可选)：一个接收目标并转换它的函数

pytorch torchvision transfrom 图形变换，可以用Compose将多个transform组合起来使用
transfrom = transfroms.Compose([transfroms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),])
transforms.Normalize(mean, std):给定均值(R,G,B)，方差(R,G,B), 将会把Tensor正则化。即:Normalized_image=(image-mean)/std
  >mean(sequence):序列R,G,B的均值
  >std(sequence):序列R,G,B的平均标准偏差
  >返回结果：规范化的图片
  >返回样式：tensor张量
transforms.ToTensor:把一个取值范围是[0,255]的PIL.Image或者shape为(H,W,C)的numpy.ndarray,转换为形状为[C,H,W],取值范围是[0,1.0]的torch.FloadTensor
  >返回结果：转换后的图像
  >返回样式：Tensor张量
  
torch.nn.Linear(in_features, out_features, bias=True)
对输入数据做线性变换： y = Ax + b
  >in_features:每个输入样本的大小
  >out_featrues:每个输出样本的大小
  >bias:若设置为False,这层不会学习偏置。默认值：True
形状：
  >输入：6(N, in_features) #N为训练数据个数
  >输出：(N, out_feature)  #N为输出结果个数
linear = torch.nn.linear(3,2)
变量：
  >linear.weight:形状为(out_features * in_features)的模块中可学习的权值
  >linear.bias:形状为(out_features)的模块中可学习的偏置
linear.parameters() ——> torch.optim.SGD(linear.parameters(), lr=0.001, momentum=0.9) 传给优化器时的参数

dset.CIFAR10(root, train=True, transform=None, target_transform=None, download=False)
  >root:数据根目录
  >train:True代表训练集，False代表测试集
  >download:True=从互联网上下载数据,并将其放在root目录下，False=什么都不干

Non-Linear Activatoins
class torch.nn.ReLU(inplace=False) #对输入运用线性单元函数
  >inplace:选择是否进行覆盖运算
shape:
  >输入：代表任意数目附加维度
  >输出：与输入拥有同样的shape属性

torch.nn.BatchNorm2d(num_features, eps=le-05, momentum=0.1, affine=True)
对小批量(mini-batch)3d数据组成的4d输入，进行批标准化操作：（公式看文档）
在每一个小批量数据中，计算输入各个维度的均值和标准差，gamma与beta是可学习的大小为C的参数向量(C输入大小)
在训练时，该层计算每次输入的均值和方差，并进移动平均。移动平均默认的动量值为0.1
在验证时，训练求得的均值/方差用于标准化验证数据
  >num_features:来自期望输入的特征数，该期望输入的大小为batch_size * num_features * height * width
  >eps:为保证数值稳定性(分母不能趋近或取0)，给分母加上的值。默认为le-5
  >momentum:动态均值和动态方差所使用的动量。默认为0.1
  >affine:一个布尔值。当设为true，给该层添加可学习的仿射变换参数。
Shape: -输入：(N, C, H, W)
       -输出：(N, C, H, W)

torch.nn.LSTM(*args, **kwargs)
参数说明：
  >input_size:输入的特征维度
  >hidden_size:隐藏状态的特征维度
  >num_layers:层数（和时序展开要区分开）
  >bias:如果为False，那么LSTM将不会使用偏置b_ih和b_hh。默认为True
  >batch_first:因为nn.lstm()接受的数据输入是（序列长度，batch，输入维度），这和我们cnn的输入方式
               不太一致。如果为True，那么输入和输出的Tensor的形状为(batch, seq, feature
  >dropout:若果非0的话，将会在RNN的输出上加个dropout，最后一层除外
  >bidirectional:如果为True，将会变成一个双向RNN。默认为False
LSTM输入：input,(h_0, c_0)
  >input: (batch_size, sequence_length, features)，在batch_first为true的情况下
  >h0: (num_layers*num_directions, batch, hidden_size)
       第一个参数取决于网络层数是否是双向，如果双向需要乘2；如果多层需要乘以网络层数
  >c0: 三个参数和h0一致
LSTM输出：output, (h_out, c_out)
  > out, (h_out, c_out) = lstm(x, (h0, c0))，这样即可得到输出
  >output(sequence_length, batch_size, hidden_size*num_direction)保存LSTM最后一层的输出h_t
  >h_out(num_layers*num_directions, batch, hidden_size)时间t=seq_len的隐藏层状态
  >c_out(num_layers*num_directions, batch, hidden_size)时间t=seq_len的cell状态
对LSTM输入输出的理解：
 >输入：在LSTM的forward函数中，可以定义h0和c0，不定义默认都为0。不可以把LSTM的输入输出和
  LSTM中定义结构的参数搞混，但是输入的维度必须要符合定义的LSTM结构。还有input是三维的，
  第一维表示该批次训练数据共有多少个（几张图），第二维表示每个训练数据的序列长度（比如
  说，28x28的图片，28行代表有28个序列，分成28个序列传），第三维表示每个训练数据中一个
  序列中包含的特征个数（图片中的28列，代表每一个序列特征28个）。
 >输出：输出有最后一层的所有h的状态，还有h_out和c_out代表t=seq_len的单独单独状态，包含
  所有层的状态而不仅仅是第一层
